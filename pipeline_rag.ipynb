{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation pipeline RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Etape 1. Initialiser le modèle LLM\n",
    "2. Etape 2. Créer le prompt template\n",
    "3. Etape 3. Ingestion PDF\n",
    "4. Etape 4. Chunks\n",
    "5. Etape 5. Embedding, vector store et retriever\n",
    "6. Etape 6. Chain\n",
    "7. Etape 7. Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le choix des classes et méthodes a été réalisé à l'issue d'une réflexion autour de notre besoin métier et à l'issue d'une phase de pré-test. Nous avons choisi les modules nous paraissant les plus pertinents quant au ratio coût-pertinence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe PyMuPDFLoader en association avec la méthode load() charge le fichier dont elle extrait le texte brut qu'elle stocke dans une variable objet constituée d'une collection de documents (à raison d'un document par page, avec les métadonnées). Parmi les nombreux extracteurs de textes assumés par Langchain, PyMuPDFLoader est de loin le plus rapide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "pdf_path = \"plu_0.pdf\"\n",
    "docs = PyMuPDFLoader(pdf_path).load()\n",
    "\n",
    "print(len(docs))\n",
    "print(type(docs))\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La longueur de la variable docs est bien 172, soit le nombre de pages de notre PDF. C'est une liste d'objets langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Etape 2. Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594\n",
      "<class 'list'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "# création des chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "# ajout d'un filtre pour nettoyer les métadonnées - réduit la complexité\n",
    "chunks = filter_complex_metadata(chunks)\n",
    "\n",
    "print(len(chunks))\n",
    "print(type(chunks))\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons réduit la taille de notre objet à 594 chunks. La variable est également une liste d'objets langchain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Etape 3. Embedding, vector store et retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit à présent de créer la base de données vectorielle et le récupérateur qui va retrouver les chunks les plus pertinents. Pour cela, nous utiliserons le modèle pré-entraîné FAISS (implémenté par Meta), qui va stocker les vecteurs et les indexer. Nous paramétrerons ensuite l'objet issu de ce processus à l'aide de la méthode as_retriever() pour que celui-ci effectue un calcul de similarité sémantique avec la requête (par défaut : similarité cosinus), en fonction des k-voisins que nous lui indiquerons et ce à partir d'un seuil d'acceptabilité (\"score_threshold\"). Pour cela, nous utiliserons la méthode as_retriever(). Le nombre de k-voisins à sélectionner est également déterminant, il s'agira de trouver le k qui dans notre cas d'usage augmente les chances de contenir le bon document, tout en réduisant les risques de noyer l'information dans des documents non pertinents. Nous choisissons dans notre implémentation de renvoyer un seul document, en misant sur le fait que le retriever choisira d'emblée le bon. Nous ferons varier ces éléments dans la partie expérimentation. Nous choisissons la méthode de recherche par calcul du score de similarité cosinus idéale dans notre cas car nous recherchons des résultats très pertinents par rapport à notre requête (nous ne demandons surtout pas de la variabilité comme cela pourrait être calculé à partir de l'approche \"mmr\" ou Maximal Marginal Relevance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae915e5c7d09429c916101cc8b2e2e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = FastEmbedEmbeddings() # par défaut, model_name de la classe: \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "vector_store = FAISS.from_documents(documents=chunks, embedding=embeddings)\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": 1, \"score_threshold\": 0.7,},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons à présent observer le fonctionnement du retriever, grâce à la méthode invoke() qui déclenche le processus de récupération, si on lui donne directement la requête comme une chaîne de caractères."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'plu_0.pdf', 'file_path': 'plu_0.pdf', 'page': 8, 'total_pages': 172, 'format': 'PDF 1.4', 'title': 'GIG_PLU_Modif1_APPRO_4a_Reglt_20210628', 'author': 'Eloïse DE CARVALHO', 'subject': '', 'keywords': '', 'creator': 'Word', 'producer': 'Mac OS X 10.12.6 Quartz PDFContext', 'creationDate': \"D:20210628092938Z00'00'\", 'modDate': \"D:20210628092938Z00'00'\", 'trapped': ''}, page_content='la construction ou de l’installation, cheminées, antennes et autres ouvrages techniques exclus. \\n2) Hauteur maximum au faîtage \\nToute construction ou installation ne peut excéder 12,50 mètres de «hauteur  maximum». \\nEn cas d’extension ou de rénovation de bâtiments existant ayant une hauteur supérieure au maximum \\nindiqué ci dessus, la hauteur pourra atteindre celle de la construction existante. \\n3) Hauteur relative \\nPar rapport à la voie, la hauteur de toute constructions doit être telle que la différence d’altitude entre \\ntout point du bâtiment et tout point de l’alignement opposé n’excède pas le double de la distance comptée \\nhorizontalement entre ces deux points, sans que cette hauteur ne puisse être supérieure à la hauteur \\nmaximale autorisée. Le schéma ci-dessous explicite cette règle : \\n \\n \\n \\n> Article 11 :  Aspect extérieur \\nLes constructions doivent présenter un aspect compatible avec le caractère ou l’intérêt des lieux avoisi-')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_chunks = retriever.invoke(\"Quelle est la hauteur maximale des constructions dans la zone U1 ?\")\n",
    "retrieved_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous observons que le chunk retrouvé correspond bien à celui qui contient l'information désirée, à savoir 12.50 mètres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Etape 4. Initialisation du LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons un LLM fourni par la plateforme Ollama qui permet de charger divers modèles de langage dans un environnement local, que Langchain prend ensuite en charge. Ici nous sélectionnons Llama2, mais nous testerons différents modèles dans notre partie expérimentation. Il est important de présenter les paramètres qui vont contrôler le comportement du modèle.\n",
    "- temperature : si elle est proche de 0, le modèle reste déterministe, produisant toujours les réponses les plus probables, admettant peu de diversité ou de créativité. En revanche, si elle est plus élevée (1, 2 ou 3), la température rend le modèle plus créatif, au détriment parfois de la cohérence. Dans notre cas, nous avons impérativement besoin de réponses précises, sans hallucinations, et reproductibles. Nous choisissons donc d'emblée une température de 0, et nous ne ferons plus varier ce paramètre ensuite.\n",
    "- top_k : détermine combien d'options (k) parmi les plus probables le LLM choisira lors de la génération de chaque token. Ici aussi nous choississons une valeur faible car nous voulons des chiffres et de la précision.\n",
    "- top_p : sélectionne les tokens suivants selon une distribution de probabilité. Nous utilisons une valeur de 1 (soit 100%) pour prendre en compte toutes les possibilités de réponses au moment de la génération. Associé à la température de 0 et au top_k de 1, ce paramètre nous semble favorable pour maximiser les réponses précises et pertinentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama2\", temperature=0, top_k=1, top_p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Etape 5. Création du prompt template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le prompt permet de guider le LLM dans sa manière d'interagir avec le contexte et de structurer sa réponse. Encore une fois, il nous faut priviléger un prompt à l'image de ce que nous attendons de la génération : de la concision et de la précision.\n",
    "Pour une documentation sur les bonnes pratiques en matière de formulation de prompt (hors RAG), voir https://medium.com/the-modern-scientist/best-prompt-techniques-for-best-llm-responses-24d2ff4f6bca et \n",
    "https://www.promptingguide.ai/introduction/examples#information-extraction\n",
    "\n",
    "Il a été montré qu'une invite simple et précise augmente la qualité de la réponse https://arxiv.org/pdf/2312.16171."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    <s> [INST] Vous êtes un assistant chargé de l'analyse des documents d'urbanisme. \n",
    "    Votre tâche est d'extraire des chiffres à partir du contexte.\n",
    "    Donnez uniquement le chiffre, rien d'autre. \n",
    "    Exemple : \n",
    "    Question : Quelle est la hauteur maximale des constructions dans la zone U1 ?\n",
    "    Réponse : 12.5\n",
    "    [/INST] </s> \n",
    "    [INST] Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: [/INST]\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Etape 6. Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette étape concerne la création de la chaîne de traitement : langchain permet grâce à une syntaxe spécifique d'assembler toutes les étapes du pipeline en une seule commande de traitement. Nous retrouvons les 2 points clés : récupérer la requête et le contexte pertinent à partir du vector store, puis générer une réponse par le LLM. La classe RunnablePassthrough permet de passer la question telle quelle dans la chaîne et StrOutputParser permet de parser la sortie du modèle en transformant la réponse générée en une chaîne de caractère."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "         | prompt_template\n",
    "         | model\n",
    "         | StrOutputParser()\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Etape 7. Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12.5\n"
     ]
    }
   ],
   "source": [
    "question = \"Quelle est la hauteur maximale des constructions dans la zone U1 ?\"\n",
    "answer = chain.invoke(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temps de réponse du LLM : 4 min 32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classe complète et utilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_community.embeddings import FastEmbedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryPDF:\n",
    "    vector_store = None\n",
    "    retriever = None\n",
    "    chain = None\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = ChatOllama(model=\"llama2\")\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "        self.prompt = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            <s> [INST] Vous êtes un assistant chargé de l'analyse des documents d'urbanisme. \n",
    "            Votre tâche est d'extraire des chiffres à partir du contexte.\n",
    "            Donnez uniquement le chiffre, rien d'autre. \n",
    "            Exemple : \n",
    "            Question : Quelle est la hauteur maximale des constructions dans la zone U1 ?\n",
    "            Réponse : 12.5\n",
    "            [/INST] </s> \n",
    "            [INST] Question: {question} \n",
    "            Context: {context} \n",
    "            Answer: [/INST]\n",
    "            \"\"\"\n",
    "        )\n",
    "\n",
    "    def ingest(self, pdf_path: str):\n",
    "        docs = PyMuPDFLoader(file_path=pdf_path).load()\n",
    "        chunks = self.text_splitter.split_documents(docs)\n",
    "        chunks = filter_complex_metadata(chunks)\n",
    "\n",
    "        vector_store = FAISS.from_documents(documents=chunks, embedding=FastEmbedEmbeddings())\n",
    "        self.retriever = vector_store.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\n",
    "                \"k\": 1,\n",
    "                \"score_threshold\": 0.7,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        self.chain = ({\"context\": self.retriever, \"question\": RunnablePassthrough()}\n",
    "                      | self.prompt\n",
    "                      | self.model\n",
    "                      | StrOutputParser())\n",
    "\n",
    "    def ask(self, query: str):\n",
    "        if not self.chain:\n",
    "            return \"Please, add a PDF document first.\"\n",
    "        return self.chain.invoke(query)\n",
    "\n",
    "    def clear(self):\n",
    "        self.vector_store = None\n",
    "        self.retriever = None\n",
    "        self.chain = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7553273d2074428fac66d3d434c91a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "12.5\n"
     ]
    }
   ],
   "source": [
    "#### Utilisation de la classe ####\n",
    "\n",
    "# 1. Instanciation de la classe\n",
    "pdf_query = QueryPDF()\n",
    "\n",
    "# 2. Ingestion du PDF\n",
    "pdf_query.ingest(\"plu_0.pdf\")\n",
    "\n",
    "# 3. Question\n",
    "answer = pdf_query.ask(\"Quelle est la hauteur maximale des constructions dans la zone U1 ?\")\n",
    "print(answer)\n",
    "\n",
    "# 4. Effacer les variables stockées\n",
    "pdf_query.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_plu",
   "language": "python",
   "name": "venv_plu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
